{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 8: Line detection on the Raspberry Pi 3 B+\n",
    "\n",
    "Written by: Enrique Mireles Gutiérrez  \n",
    "ID Number: 513944  \n",
    "Bachelor: ITR  \n",
    "Date: 2019-04-01  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Until now, all proposed algorithms throughout lab reports 2 and 7 have been running in a laptop using a last generation processor. However, since the target device for the autonomous vehicle will be a Raspberry Pi 3 B+, it makes perfect sense to start migrating to this platform. Some key aspects must be taken into account when migrating. \n",
    "\n",
    "The processing power of both devices is really different. This means that some performance measurements must done in order to create a baseline to further optimize the algorithms. This will help maintain a reliable frame rate for autonomous diving. Other aspects such as software architecture play an important role during migration. Since the Raspberry Pi 3 B+ uses an ARM processor, the libraries used in the PC can’t be downloaded to a Raspberry Pi with ease. Therefore, OpenCV must be compiled from source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "This lab has the following objectives:\n",
    "- Implement the algorithms presented during the lab 07 in a Raspberry Pi 3 B+.\n",
    "- Tweak algorithms to maximize performance on target device.\n",
    "- Present results and possible solutions for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure\n",
    "\n",
    "This lab report is subdivided in smaller numbered programs shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Program\n",
    "\n",
    "This lab report contains a single program, similar to the one presented at the end of lab 07. With this in mind, no other sections are presented with runnable code (such as in other lab reports). This version performs time measurements in order to determine which settings perform faster. No other changes were made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 480x270\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "\n",
    "HIGHWAY_VIDEO = '../fig/videos/highway.mp4'\n",
    "SCALE = 0.5\n",
    "\n",
    "settings = {\n",
    "    'video_file': HIGHWAY_VIDEO,\n",
    "    'scale': SCALE,\n",
    "    'blur_kernel_size': (7, 7),\n",
    "    'blur_iterations': 1,\n",
    "    'canny_low_threshold': 10,\n",
    "    'canny_high_threshold': 40,\n",
    "    'hough_rho': 1,\n",
    "    'hough_threshold': 20,\n",
    "    'hough_theta': np.pi / 180,\n",
    "    'hough_min_line_length': 5,\n",
    "    'hough_max_line_gap': 60,\n",
    "    'show_all_hough_lines': False,\n",
    "    'show_canny': False,\n",
    "    'roi_vertices': np.array([[\n",
    "        (430, 830),           # Bottom left roi coordinate.\n",
    "        (900, 600),           # Top left roi coordinate.\n",
    "        (1020, 600),          # Top Right roi coordinate.\n",
    "        (1530, 830)           # Bottom right roi coordinate.\n",
    "    ]]) * 0.5 * SCALE,\n",
    "    'abs_min_line_angle': 20,\n",
    "    'lane_min_y': int(600 * 0.5 * SCALE),\n",
    "    'lane_max_y': int(830 * 0.5 * SCALE)\n",
    "}\n",
    "\n",
    "def isGrayscale(img):\n",
    "    \"\"\"\n",
    "        Returns true if the image is grayscale (channels == 1).\n",
    "        Returns false if channels > 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If img.shape has a channel value, read it and determine if\n",
    "    # it is a grayscale image. If it doesn't have, assume that the\n",
    "    # image is grayscale.\n",
    "    if (len(img.shape) == 3):\n",
    "        return img.shape[2] == 1\n",
    "    return True\n",
    "\n",
    "def createRoi(img, vertices):\n",
    "    \"\"\"\n",
    "        Applies an image mask.\n",
    "\n",
    "        Only keeps the region of the image defined by the polygon\n",
    "        formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)\n",
    "\n",
    "    # defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if isGrayscale(img):\n",
    "        ignore_mask_color = 255\n",
    "    else:\n",
    "        ignore_mask_color = (255, 255, 255)\n",
    "\n",
    "    # filling pixels inside the polygon defined by \"vertices\" with the fill color\n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "\n",
    "    # returning the image only where mask pixels are nonzero\n",
    "    return cv2.bitwise_and(img, mask)\n",
    "\n",
    "def findLanes(hough_lines, img, settings):\n",
    "    \"\"\"\n",
    "        Given an array of hough lines, detects which lines correspond to the left and right lane.\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty arrays for storing line coordinates (sets of points (x, y)).\n",
    "    right_lines = {\n",
    "        'x': [],\n",
    "        'y': []\n",
    "    }\n",
    "    left_lines = {\n",
    "        'x': [],\n",
    "        'y': []\n",
    "    }\n",
    "    \n",
    "    # Stores the fitted line coordinates for both lanes.\n",
    "    # Its format is [x0, y0, x1, y1].\n",
    "    lanes = {\n",
    "        'left': [],\n",
    "        'right': []\n",
    "    }\n",
    "    \n",
    "    # Make sure at least some lines have been detected.\n",
    "    if (type(hough_lines) == type(np.array([]))):\n",
    "\n",
    "        # Iterate through every line.\n",
    "        for line in hough_lines:\n",
    "\n",
    "            # Usualy every line object contains a single set of coordinates,\n",
    "            # nonetheless, it is placed inside a for for safety.\n",
    "            for x1, y1, x2, y2 in line:\n",
    "\n",
    "                # Calculate the direction of the line found.\n",
    "                direction = np.rad2deg(np.arctan2(y2 - y1, x2 - x1))\n",
    "\n",
    "                # Only draw lines whose angle is greater than the threshold.\n",
    "                if (np.abs(direction) > settings['abs_min_line_angle']):\n",
    "\n",
    "                    # If lines have a positive direction they are from the\n",
    "                    # right lane.\n",
    "                    if (direction > 0):\n",
    "                        # Right lane.\n",
    "                        right_lines['x'].extend([x1, x2])\n",
    "                        right_lines['y'].extend([y1, y2])\n",
    "                        if (settings['show_all_hough_lines']): \n",
    "                            cv2.line(img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
    "                    else:\n",
    "                        # Left lane.\n",
    "                        left_lines['x'].extend([x1, x2])\n",
    "                        left_lines['y'].extend([y1, y2])\n",
    "                        if (settings['show_all_hough_lines']):\n",
    "                            cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 1)\n",
    "                    \n",
    "                    # Write the angle of the line for debugging purposes.\n",
    "                    if (settings['show_all_hough_lines']): cv2.putText(img, '%.1f' % (direction), (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(255, 255, 0), thickness=1)\n",
    "    \n",
    "    # Make sure points on the left side were found.\n",
    "    if (len(left_lines['x']) > 0 and len(left_lines['y']) > 0) and not settings['show_all_hough_lines']:\n",
    "        \n",
    "        # Using the points found, find a 1st order polynomial that best fits the data.\n",
    "        poly_left = np.poly1d(np.polyfit(left_lines['y'], left_lines['x'], deg=1))\n",
    "        \n",
    "        # Evaluate the function found for the desired lane lengths.\n",
    "        lanes['left'] = [\n",
    "            int(poly_left(settings['lane_min_y'])),\n",
    "            settings['lane_min_y'],\n",
    "            int(poly_left(settings['lane_max_y'])),\n",
    "            settings['lane_max_y']\n",
    "        ]\n",
    "    \n",
    "    # Make sure points on the right side were found.\n",
    "    if (len(right_lines['x']) > 0 and len(right_lines['y']) > 0) and not settings['show_all_hough_lines']:\n",
    "        \n",
    "        # Using the points found, find a 1st order polynomial that best fits the data.\n",
    "        poly_right = np.poly1d(np.polyfit(right_lines['y'], right_lines['x'], deg=1))\n",
    "        \n",
    "        # Evaluate the function found for the desired lane lengths.\n",
    "        lanes['right'] = [\n",
    "            int(poly_right(settings['lane_min_y'])), \n",
    "            settings['lane_min_y'], \n",
    "            int(poly_right(settings['lane_max_y'])), \n",
    "            settings['lane_max_y']\n",
    "        ]\n",
    "\n",
    "    return lanes\n",
    "\n",
    "def drawLanes(lanes, img):\n",
    "    \"\"\"\n",
    "        Given the output of findLanes, draws the lanes in the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    if (len(lanes['left'])):\n",
    "        cv2.line(img, (lanes['left'][0], lanes['left'][1]), (lanes['left'][2], lanes['left'][3]), (0, 0, 255), 2)\n",
    "    \n",
    "    if (len(lanes['right'])):\n",
    "        cv2.line(img, (lanes['right'][0], lanes['right'][1]), (lanes['right'][2], lanes['right'][3]), (0, 0, 255), 2)\n",
    "\n",
    "def findLines(img, settings):\n",
    "    \"\"\"\n",
    "        Given an BGR image, finds all lines in the image using a Probabilistic Hough Transform.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to gray scale.\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Blur image so that the Canny edge detector doesn't find useless edges.\n",
    "    blur_gray = gray\n",
    "    for i in range(settings['blur_iterations']):\n",
    "        blur_gray = cv2.GaussianBlur(blur_gray, (settings['blur_kernel_size']), sigmaX=0, sigmaY=0)\n",
    "    \n",
    "    # Detect edges and create a ROI for the section defined.\n",
    "    edges = cv2.Canny(blur_gray, settings['canny_low_threshold'], settings['canny_high_threshold'], apertureSize=3)\n",
    "    masked_edges = createRoi(edges, settings['roi_vertices'].astype(int))\n",
    "    \n",
    "    # Detect lines.\n",
    "    hough_lines = cv2.HoughLinesP(\n",
    "        masked_edges, \n",
    "        rho = settings['hough_rho'], \n",
    "        theta = settings['hough_theta'], \n",
    "        threshold = settings['hough_threshold'], \n",
    "        lines = np.array([]), \n",
    "        minLineLength = settings['hough_min_line_length'], \n",
    "        maxLineGap = settings['hough_min_line_length']\n",
    "    )\n",
    "    \n",
    "    # Return all data.\n",
    "    return  {\n",
    "        'gray': gray,\n",
    "        'blur_gray': blur_gray,\n",
    "        'edges': edges,\n",
    "        'masked_edges': masked_edges,\n",
    "        'hough_lines': hough_lines\n",
    "    }\n",
    "\n",
    "# Open selected video file.\n",
    "cap = cv2.VideoCapture(settings['video_file'])\n",
    "\n",
    "# Get scaled video properties.\n",
    "FRAME_WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) * settings['scale'])\n",
    "FRAME_HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) * settings['scale'])\n",
    "FRAME_FPS = cap.get(cv2.CAP_PROP_FPS)\n",
    "print('Dimensions: %dx%d' % (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "# Create input and ROI windows. Place each window side by side.\n",
    "cv2.namedWindow('input', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.moveWindow('input', 0, 200)\n",
    "if (settings['show_canny']):\n",
    "    cv2.namedWindow('ROI', cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.moveWindow('ROI', FRAME_WIDTH, 200)\n",
    "\n",
    "# Open video.\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    # Read frame form video.\n",
    "    t0 = cv2.getTickCount()\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # If video ended exit.\n",
    "    if ret:\n",
    "        \n",
    "        # Resize frame to desired scale.\n",
    "        if (settings['scale'] != 1):\n",
    "            img = cv2.resize(frame, (0, 0), fx=settings['scale'], fy=settings['scale'])\n",
    "        else:\n",
    "            img = frame\n",
    "        \n",
    "        # Find all lines in the image.\n",
    "        e1 = cv2.getTickCount()\n",
    "        output = findLines(img, settings)\n",
    "        e2 = cv2.getTickCount()\n",
    "        dt_findLines = (e2 - e1) / cv2.getTickFrequency()\n",
    "        \n",
    "        # With the lines found, find which of them belong to the left and right lanes.\n",
    "        e1 = cv2.getTickCount()\n",
    "        lanes = findLanes(output['hough_lines'], img, settings)\n",
    "        e2 = cv2.getTickCount()\n",
    "        dt_findLanes = (e2 - e1) / cv2.getTickFrequency()\n",
    "        \n",
    "        # Draw results.\n",
    "        drawLanes(lanes, img)\n",
    "        \n",
    "        # Calculate loop time.\n",
    "        e2 = cv2.getTickCount()\n",
    "        dt = (e2 - t0) / cv2.getTickFrequency()\n",
    "        \n",
    "        # Draw times to output image.\n",
    "        cv2.putText(img, 'Cycle: %.4fms - %.1ffps' % (dt, 1 / dt), (10, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        cv2.putText(img, 'findLines: %.4fms' % (dt_findLines), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        cv2.putText(img, 'findLanes: %.4fms' % (dt_findLanes), (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        \n",
    "        cv2.imshow('input', img)\n",
    "        if (settings['show_canny']):\n",
    "            cv2.imshow('ROI', output['masked_edges'])\n",
    "\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "    # If 'q' is pressed, then exit.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# Clear memory and windows.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Results\n",
    "\n",
    "The performance measurements in the Raspberry Pi were made by changing the scale of the video and reporting the time it took to process the frame. These values were displayed onto the frame and included:\n",
    "- **cycle time** and **frame rate** (total time between frames)\n",
    "- **findLines time** (time required to perform a Hough Transform including the preprocessing required for it.)\n",
    "- **findLanes time** (time required to determine which lines correspond to the left and right lanes and then fit a 1st order polynomial to the data.)\n",
    "\n",
    "The scale of the video was changed between a 100%, 50%, 35%, 30% and 25%. These percentages yielded a wide variety of results shown in the following table. All times are presented in milliseconds (ms).\n",
    "\n",
    "| Scale | Width | Height |  FPS | Cycle [ms] | findLines [ms] | findLanes [ms] | Total Algorithm [ms] | Frame Capture [ms] |\n",
    "|:-----:|:-----:|:------:|:----:|:-----:|:---------:|:---------:|:---------------:|:-------------:|\n",
    "|   1   |  960  |   540  | 10.2 |  97.7 |    70.7   |    5.2    |       75.9      |      21.8     |\n",
    "|  0.5  |  480  |   270  | 18.6 |  53.9 |    22.9   |    5.5    |       28.4      |      25.5     |\n",
    "|  0.35 |  336  |   189  | 19.4 |  51.5 |    13.6   |    4.6    |       18.2      |      33.3     |\n",
    "|  0.3  |  228  |   162  |  22  |  45.4 |    11.7   |    4.8    |       16.5      |      28.9     |\n",
    "|  0.25 |  240  |   135  | 24.6 |  40.6 |    9.4    |     5     |       14.4      |      26.2     |\n",
    "\n",
    "###### Figure 1. Results at 100% Scale\n",
    "<p align='center'>\n",
    "    <img src='scale-100.png' alt='Video Scale 100%'>\n",
    "</p>\n",
    "\n",
    "###### Figure 2. Results at 50% Scale\n",
    "<p align='center'>\n",
    "    <img src='scale-50.png' alt='Video Scale 50%'>\n",
    "</p>\n",
    "    \n",
    "###### Figure 3. Results at 35% Scale\n",
    "<p align='center'>\n",
    "    <img src='scale-35.png' alt='Video Scale 35%'>\n",
    "</p>\n",
    "\n",
    "###### Figure 4. Results at 30% Scale\n",
    "<p align='center'>\n",
    "    <img src='scale-30.png' alt='Video Scale 30%'>\n",
    "</p>\n",
    "\n",
    "###### Figure 5. Results at 25% Scale\n",
    "<p align='center'>\n",
    "    <img src='scale-25.png' alt='Video Scale 25%'>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "All in all the migration was quite simple. The algorithms used in lab 07 worked flawlessly in the Raspberry Pi. By using a settings dictionary the testing process was quite simple. The scale changes were made by changing a variable and all other dimensions scaled accordingly. It was found that every window being displayed resulted in a negative impact on the time response of the system. Therefore, the canny edge output display was disabled by a setting in the dictionary.\n",
    "\n",
    "Some interesting facts arise from analyzing the table:\n",
    "- The time that the function `findLanes` uses remains pretty much constant independently from the frame size. This makes sense since the input to the function is an array of lines, which in the most part remain constant regarding on the frame size.\n",
    "- The function `findLines` depends directly on frame size. Quite a reduction is seen between 100% and 50%. At 50% the frame size is 480x270, which approximates to a more standard dimension like 480x360. At this frame size the Probabilistic Hough Transform took about 23ms to execute. This time is smaller than the time between frames (30 fps or 1/30sec); the importance of this is discussed in the following point.\n",
    "- An area of opportunity for optimizing the performance of the Raspberry Pi relies in the last column of the results table: the frame capture time. This is the time that it takes to receive a frame from the camera. The highway video has a frame rate of 29.97 or 33 ms per frame. Since the total algorithm execution time at 50% is 28.4ms, this means that if the video acquisition and video processing are separated in multiple processes, an almost real-time execution could be achieved.\n",
    "\n",
    "A next step for the migration would be to implement a threaded or multi-process application which acquires and processes video separately. Furthermore, several section of the code use nested for loops, which impact greatly on performance. Other alternatives must be used instead of for loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I hereby affirm that I have done this activity with academic integrity._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
